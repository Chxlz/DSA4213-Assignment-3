# -*- coding: utf-8 -*-
"""DSA4213_A0259291W_Assignment 3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lPCtRlml2pfW2qN-_-qwGcBwJrrnr-q4

# **DSA Assignment 3**

### **Logins**
"""

from huggingface_hub import login
login()

import wandb
wandb.login()

"""### **Install libraries and import packages**"""

# install libraries
!pip install -q transformers datasets peft accelerate evaluate torch matplotlib scikit-learn

# import
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
import evaluate
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
import pandas as pd

"""### **Load Dataset**"""

# load data
dataset = load_dataset("shawhin/phishing-site-classification")

# check data structure
print(dataset)

# see sample
print(dataset['train'][0])

"""### **Tokenize**"""

# load tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# tokenize
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=64)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.remove_columns(["text"])
tokenized_datasets.set_format("torch")

train_dataset = tokenized_datasets["train"]
val_dataset   = tokenized_datasets["validation"]
test_dataset  = tokenized_datasets["test"]

"""### **Evaluation helper function**"""

# load metrics
accuracy_metric  = evaluate.load("accuracy")
f1_metric        = evaluate.load("f1")
precision_metric = evaluate.load("precision")
recall_metric    = evaluate.load("recall")
roc_metric       = evaluate.load("roc_auc")

# evaluation helper function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)[:, 1].numpy()  # phising = 1, legit = 0

    accuracy  = accuracy_metric.compute(predictions=preds, references=labels)
    f1        = f1_metric.compute(predictions=preds, references=labels, average="macro")
    precision = precision_metric.compute(predictions=preds, references=labels, average="macro")
    recall    = recall_metric.compute(predictions=preds, references=labels, average="macro")
    roc_auc   = roc_metric.compute(prediction_scores=probs, references=labels)

    return {
        "accuracy":  accuracy["accuracy"],
        "macro_f1":  f1["f1"],
        "precision": precision["precision"],
        "recall":    recall["recall"],
        "roc_auc":   roc_auc["roc_auc"],
    }

"""### **Train Full Finetune BERT**"""

# load pretrained BERT
model_full = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# training arguments
training_args_full = TrainingArguments(
    output_dir="results_full",
    eval_strategy="epoch",
    save_strategy="no",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="logs_full",
)

# trainer object
trainer_full = Trainer(
    model=model_full,
    args=training_args_full,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

# train
trainer_full.train()

"""### **Train LoRA (Rank=4) Fine-Tune BERT**"""

# load pretrained BERT
model_lora4 = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# lora rank 4
lora_config4 = LoraConfig(
    r=4, lora_alpha=16, target_modules=["query", "value"],
    lora_dropout=0.05, bias="none", task_type="SEQ_CLS"
)
model_lora4 = get_peft_model(model_lora4, lora_config4)

# training arguments
training_args_lora4 = TrainingArguments(
    output_dir="results_lora4",
    eval_strategy="epoch",
    save_strategy="no",
    learning_rate=1e-4,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="logs_lora4",
)

# trainer object
trainer_lora4 = Trainer(
    model=model_lora4,
    args=training_args_lora4,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

# train
print("training lora r=4 model...")
trainer_lora4.train()

"""### **Train LoRA (Rank=8) Fine-Tune BERT**"""

# load pretrained BERT
model_lora8 = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# lora rank 8
lora_config8 = LoraConfig(
    r=8, lora_alpha=16, target_modules=["query", "value"],
    lora_dropout=0.05, bias="none", task_type="SEQ_CLS"
)
model_lora8 = get_peft_model(model_lora8, lora_config8)

# training arguments
training_args_lora8 = TrainingArguments(
    output_dir="results_lora8",
    eval_strategy="epoch",
    save_strategy="no",
    learning_rate=1e-4,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="logs_lora8",
)

# training object
trainer_lora8 = Trainer(
    model=model_lora8,
    args=training_args_lora8,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

# train
print("training lora r=8 model...")
trainer_lora8.train()

"""### **Train LoRA (Rank=16) Fine-Tune BERT**"""

# load pretrained BERT
model_lora16 = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# lora rank 16
lora_config16 = LoraConfig(
    r=16, lora_alpha=16, target_modules=["query", "value"],
    lora_dropout=0.05, bias="none", task_type="SEQ_CLS"
)
model_lora16 = get_peft_model(model_lora16, lora_config16)

# training arguments
training_args_lora16 = TrainingArguments(
    output_dir="results_lora16",
    eval_strategy="epoch",
    save_strategy="no",
    learning_rate=1e-4,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="logs_lora16",
)

# training object
trainer_lora16 = Trainer(
    model=model_lora16,
    args=training_args_lora16,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

# train
print("training lora r=16 model...")
trainer_lora16.train()

"""### **Evaluate Models**"""

# evaluate all trained models on test set
print("evaluating full fine-tuning model...")
full_results = trainer_full.evaluate(test_dataset)
print(full_results)

print("\nevaluating lora r=4 model...")
lora4_results = trainer_lora4.evaluate(test_dataset)
print(lora4_results)

print("\nevaluating lora r=8 model...")
lora8_results = trainer_lora8.evaluate(test_dataset)
print(lora8_results)

print("\nevaluating lora r=16 model...")
lora16_results = trainer_lora16.evaluate(test_dataset)
print(lora16_results)

# create results table
data = {
    "model": ["full fine-tune", "lora r=4", "lora r=8", "lora r=16"],
    "accuracy": [full_results["eval_accuracy"], lora4_results["eval_accuracy"], lora8_results["eval_accuracy"], lora16_results["eval_accuracy"]],
    "f1": [full_results["eval_macro_f1"], lora4_results["eval_macro_f1"], lora8_results["eval_macro_f1"], lora16_results["eval_macro_f1"]],
    "precision": [full_results["eval_precision"], lora4_results["eval_precision"], lora8_results["eval_precision"], lora16_results["eval_precision"]],
    "recall": [full_results["eval_recall"], lora4_results["eval_recall"], lora8_results["eval_recall"], lora16_results["eval_recall"]],
    "roc_auc": [full_results["eval_roc_auc"], lora4_results["eval_roc_auc"], lora8_results["eval_roc_auc"], lora16_results["eval_roc_auc"]],
}

df = pd.DataFrame(data)
print(df.round(4))

# plot roc curves for all 4 models
def get_probs_and_labels(trainer, dataset):
    preds = trainer.predict(dataset)
    logits = preds.predictions
    labels = preds.label_ids
    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)[:, 1].numpy()
    return probs, labels

# get predicted probabilities for each model
probs_full,  labels = get_probs_and_labels(trainer_full,  test_dataset)
probs_lora4, _      = get_probs_and_labels(trainer_lora4, test_dataset)
probs_lora8, _      = get_probs_and_labels(trainer_lora8, test_dataset)
probs_lora16, _     = get_probs_and_labels(trainer_lora16, test_dataset)

# compute roc curves
fpr_full,  tpr_full,  _ = roc_curve(labels, probs_full)
fpr_lora4, tpr_lora4, _ = roc_curve(labels, probs_lora4)
fpr_lora8, tpr_lora8, _ = roc_curve(labels, probs_lora8)
fpr_lora16, tpr_lora16, _ = roc_curve(labels, probs_lora16)

# compute auc scores
roc_auc_full  = auc(fpr_full,  tpr_full)
roc_auc_lora4 = auc(fpr_lora4, tpr_lora4)
roc_auc_lora8 = auc(fpr_lora8, tpr_lora8)
roc_auc_lora16 = auc(fpr_lora16, tpr_lora16)

# plot all roc curves
plt.figure(figsize=(6,5))
plt.plot(fpr_full,  tpr_full,  label=f"full fine-tune (auc={roc_auc_full:.3f})")
plt.plot(fpr_lora4, tpr_lora4, label=f"lora r=4 (auc={roc_auc_lora4:.3f})")
plt.plot(fpr_lora8, tpr_lora8, label=f"lora r=8 (auc={roc_auc_lora8:.3f})")
plt.plot(fpr_lora16, tpr_lora16, label=f"lora r=16 (auc={roc_auc_lora16:.3f})")
plt.plot([0,1],[0,1],'k--')
plt.xlabel("false positive rate")
plt.ylabel("true positive rate")
plt.title("roc curves for phishing url detection")
plt.legend()
plt.show()

"""### **Errors Analysis: Comparing LoRA (r=16) Misclassifications vs Full Fine-Tune**"""

# helper to get predictions and labels
def get_preds_and_labels(trainer, dataset):
    preds = trainer.predict(dataset)
    logits = preds.predictions
    labels = preds.label_ids
    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()
    preds_cls = np.argmax(probs, axis=-1)
    return preds_cls, labels, probs

# get predictions for both models
full_preds, full_labels, full_probs = get_preds_and_labels(trainer_full, test_dataset)
lora16_preds, _, lora16_probs = get_preds_and_labels(trainer_lora16, test_dataset)

# identify where full fine-tune correct, LoRA wrong
mask = (full_preds == full_labels) & (lora16_preds != full_labels)

# convert test set back to DataFrame for inspection
test_texts = dataset["test"]["text"]
test_labels = dataset["test"]["labels"]

df_errors = pd.DataFrame({
    "text": np.array(test_texts)[mask],
    "true_label": np.array(test_labels)[mask],
    "full_pred": full_preds[mask],
    "lora_pred": lora16_preds[mask],
    "full_prob_phish": full_probs[mask][:,1],
    "lora_prob_phish": lora16_probs[mask][:,1],
})

print("Number of samples where Full FT correct but LoRA r=16 wrong:", len(df_errors))
df_errors

